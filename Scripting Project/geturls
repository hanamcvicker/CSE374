# !/bin/bash
# Hana McVicker
# 11/14/22
# Project 4 Part 1

# This script extracts URLs for C code files from the main webpage and writes them into a text file.
# This script takes in two arguments: The name of the output file for the results and the name of
# the input file

# if there are fewer than 2 arguments provided, print a usage note to stderr w exit code 1
if [ "$#" -lt 2 ] 
then 
  echo "Usage: output filename html filename " 1>&2
  exit 1
fi

# Check if inputted html file exists , print error message if  it does not
if [ ! -f "$2" ]
then
   echo "./geturls error: $2 does not exist " 1>&2
   exit 1
fi

# If output file  does not exist,create it
if [ ! -f "$1" ]
then
    touch "$1"
    
# file already exists, file is replaced
elif [ -f "$1" ]
then
   echo "./geturls warning: $1 already exists " 1>&2
    > "$1"
fi

# looks inside url of html files to extract only .c files ( <a>href = only .c files <a>)
# to the output file in the first argument provided
cat $2 | grep -Eoi '<a [^>]+>' | grep -Eo 'href="[^\"]+.c"' > "$1"

# use sed to get the right output structure of the url and replace html code in the output file
# from the previous grep statement
sed -i -e 's|href="..|https://courses.cs.washington.edu/courses/cse374/22au|' -e 's|.$||' "$1"
exit 0

